## **MuSES v0: Initial Implementation & Baseline**

**Document Version:** 0.1
**Date:** August 9, 2025

### 1. The Idea 💡

The core idea of the **Multi-Scale Embedding System (MuSES)** is to create a more sophisticated representation of text than standard embedding models. Instead of generating a single vector for a piece of text, MuSES produces embeddings at multiple levels of granularity: **token, sentence, and document**. This hierarchical structure is designed to capture both fine-grained detail and high-level concepts, mirroring how humans understand information in context. The goal is to create embeddings where the hierarchical relationships are mathematically preserved (e.g., token embeddings aggregate to form sentence embeddings).

### 2. The Approach ⚙️

This initial version (v0) validates the core training architecture. The approach is as follows:

* **Base Model**: We use a pre-trained `all-MiniLM-L6-v2` from the `sentence-transformers` library as the foundation.
* **Projection Heads**: Three separate linear layers (`nn.Linear`) are added on top of the base model. Each head is responsible for projecting the base model's output into a different target dimension for the token, sentence, and document levels respectively.
* **Hierarchical Contrastive Loss**: A custom loss function, `FullHierarchicalLoss`, is used to train the projection heads. It operates on two levels:
    1.  **Token-to-Sentence Loss**: Pushes the embedding of each token to be closer to the embedding of its parent sentence than to other sentences in the batch.
    2.  **Sentence-to-Document Loss**: Pushes the embedding of each sentence to be closer to the embedding of its parent document than to other documents in the batch.
* **Data Structure**: The SNLI dataset is used as a source of sentences. These sentences are artificially grouped into "documents" of 4 sentences each to create a hierarchical structure for training.

### 3. Key Components 📦

The v0 implementation consists of these main components working together:

* **`MultiScaleEncoder(nn.Module)`**: The primary PyTorch model that encapsulates the base `SentenceTransformer` and the three projection heads. Its forward pass generates the three scales of embeddings.
* **`FullHierarchicalLoss(nn.Module)`**: The custom loss module that calculates the combined token-to-sentence and sentence-to-document contrastive losses using an InfoNCE-style formulation.
* **`prepare_data()`**: A data loading function that sources sentences from `datasets` and structures them into the `[sentence, parent_document]` format required for training.
* **`calculate_hierarchical_consistency()`**: An evaluation metric designed to measure the model's ability to maintain hierarchy. It calculates the accuracy of matching a sentence to its correct parent document within a batch.

### 4. Current Performance 📊

The model was evaluated on the Semantic Textual Similarity (STS) benchmark. The goal of this initial test is not to outperform the baseline, but to establish a quantitative starting point.

| Model | STS Benchmark Score (Spearman) |
| :--- | :--- |
| **Baseline** (`all-MiniLM-L6-v2`) | **0.8203** |
| **MuSES v0** | 0.5422 |

The current performance is approximately **0.28 points below the baseline**. This is an expected result for a v0 model with a novel architecture and a makeshift dataset. It confirms that the training loop is functioning and the model is learning, but significant improvements are needed to make it competitive.

### 5. Future Plans 🚀

The v0 results provide a clear direction for the next iteration. The primary focus will be on improving the training signal and refining the learning process.

1.  **Transition to a Hierarchical Dataset (Wikipedia)**: The current method of artificially grouping sentences from SNLI is a major limitation. The next step is to use a dataset with a natural hierarchy, like **Wikipedia**. We can parse articles into `section -> paragraph -> sentence` structures, providing the model with authentic, high-quality hierarchical data to learn from. This should provide a much stronger and more accurate training signal.

2.  **Loss Function Enhancements**: The current contrastive loss function is simple. We can explore more advanced techniques to improve its effectiveness. One key idea is incorporating **hard negatives**. Instead of just contrasting a sentence with random sentences from other documents, we can contrast it with other sentences from the *same* document. This would force the model to learn finer-grained distinctions about a sentence's specific role within its local context.

3.  **Architectural & Hyperparameter Tuning**: Now that the basic structure is in place, we can begin experimenting.
    * **Aggregation Method**: The document embedding is currently a simple mean pooling of its text. We could explore more sophisticated methods, like an **attention-based mechanism** to create a weighted average of sentence embeddings to form the document vector.
    * **Hyperparameters**: Key parameters like the `temperature` in the loss function and the `learning_rate` should be systematically tuned to find optimal values for this specific task.