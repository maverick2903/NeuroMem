## **MuSES v2: Engineering a Superior Training Pipeline**

**Document Version:** 2.0
**Date:** August 10, 2025

### 1. The Idea üí°

The mission of the **Multi-Scale Embedding System (MuSES)** remains constant: to build a hierarchical embedding space where **token, sentence, and document** representations are explicitly linked. The goal is to move beyond flat, single-scale embeddings to a more nuanced and powerful model of textual understanding.

### 2. Evolution from v1 to v2 üìà

The development of v2 was driven by a key hypothesis: that performance was not limited by the model itself, but by the *inefficiency of the training process*. The standard random-sampling `DataLoader` was not providing consistently structured data, weakening the effect of the hierarchical loss function.

* **Iteration 1: The Architectural Breakthrough**
    * **Action**: A sophisticated new training pipeline was engineered. This included:
        1.  A `DocGroupedDataset` to organize sentences by their parent document.
        2.  A **Grouped Sampling** loop to ensure every batch contained `P` sentences from `K` documents, making the sentence-to-document loss consistently active.
        3.  A **Differential Learning Rate** optimizer to apply a small LR to the model's backbone and a larger LR to the new projection heads.
    * **Result**: An immediate and significant performance jump from 0.6771 to **0.7327**.
    * **Key Insight**: This proved that the training architecture was a critical bottleneck. Engineering a pipeline that fed the model structured, hierarchical data was the key to unlocking a new level of performance.

* **Iteration 2: Probing the Limits of Specialization**
    * **Action**: With the new, stable training process, the number of epochs was increased from 1 to 3 to test for benefits from longer training.
    * **Result**: The STS score slightly *decreased* to **0.7123**.
    * **Key Insight**: A crucial discovery. Even with a superior pipeline, the model begins to overfit to the custom hierarchical task after a single epoch. Its general-purpose knowledge starts to degrade as it over-specializes, harming its performance on the broad STS benchmark.

* **Iteration 3: The "Sweet Spot" Confirmation**
    * **Action**: Based on the overfitting insight, the amount of fine-tuning was reduced by cutting the training data from 500 to just 200 Wikipedia articles, while keeping the epoch count at 1.
    * **Result**: The STS score reached its highest point yet: **0.7335**.
    * **Key Insight**: This confirmed the existence of a "sweet spot" for fine-tuning. A smaller, targeted dose of high-quality, structured data was optimal. It provided just enough signal to learn the hierarchical task without causing the model to forget its powerful, pre-existing knowledge.

### 3. The v2 Approach & Components ‚öôÔ∏è

The v2 model's success is defined by a highly engineered and precise training methodology. This represents a significant evolution from the brute-force approach of v1.

* **`DocGroupedDataset` & Grouped Sampling**: This is the core innovation of v2. By replacing the standard `DataLoader` with a system that guarantees each batch contains structured `K`-docs-by-`P`-sentences, we ensure the sentence-to-document loss is potent and effective in every single training step.
* **Two-LR Optimizer**: The use of differential learning rates is a precision tool. It protects the invaluable knowledge in the pretrained backbone from being overwritten, while allowing the new, randomly-initialized projection heads to learn the hierarchical task quickly.
* **The "Targeted Dose" Regimen**: The v2 methodology codifies the "less is more" discovery. The optimal recipe involves fine-tuning on a **small, high-quality dataset (200 articles) for exactly one epoch**. This provides the perfect balance between learning the new task and retaining general-purpose capabilities.

### 4. Current Performance üìä

The v2 architecture has successfully closed most of the remaining gap to the baseline model. The methodical improvements to the training pipeline have proven to be more effective than simple hyperparameter tuning alone.

| Experiment | Key Change(s) | STS Score |
| :--- | :--- | :--- |
| **Baseline** | `all-MiniLM-L6-v2` | **0.8203** |
| **MuSES v1** | Simple `DataLoader`, single LR | 0.6771 |
| **v2-Iter. 1** | Implemented Grouped Sampling & Dual LR (1 epoch, 500 articles) | 0.7327 |
| **v2-Iter. 2** | Increased epochs to 3 | 0.7123 |
| **MuSES v2** | Reduced data to 200 articles (1 epoch) - **Final Result** | **0.7335** |

### 5. Future Plans (v3 and Beyond) üöÄ

With a highly optimized training pipeline, the focus can now shift back to the model's core architecture and loss functions.

1.  **Implement Attention-Based Document Embeddings**: The current document embedding is a simple mean pooling of tokens. The next major architectural step is to replace this with an **attention mechanism**. This would allow the model to learn to create document embeddings by calculating a *weighted* average of its sentence embeddings, giving more importance to key sentences. The structured batches from v2 make this approach more feasible than ever.

2.  **Activate Hard Negative Mining**: The `DocGroupedDataset` makes it trivial to implement hard negatives. For the sentence-to-document loss, we can now contrast a sentence not only with sentences from other documents (easy negatives) but also with *other sentences from its own document* (hard negatives). This should dramatically improve the model's ability to learn fine-grained semantic distinctions.

3.  **Deploy in a RAG Pipeline**: The model is now performing at a near-competitive level on a general benchmark. It's time to test it on a task that directly aligns with the goals of **NeuroMem**. Integrating MuSES into a Retrieval-Augmented Generation (RAG) system and evaluating its ability to retrieve more relevant context than a standard vector search is the ultimate test of its hierarchical power.